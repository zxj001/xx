package org.apache.spark.sql.parquet;
/**
 * :: DeveloperApi ::
 * Operator that acts as a sink for queries on RDDs and can be used to
 * store the output inside a directory of Parquet files. This operator
 * is similar to Hive's INSERT INTO TABLE operation in the sense that
 * one can choose to either overwrite or append to a directory. Note
 * that consecutive insertions to the same table must have compatible
 * (source) schemas.
 * <p>
 * WARNING: EXPERIMENTAL! InsertIntoParquetTable with overwrite=false may
 * cause data corruption in the case that multiple users try to append to
 * the same table simultaneously. Inserting into a table that was
 * previously generated by other means (e.g., by creating an HDFS
 * directory and importing Parquet files generated by other tools) may
 * cause unpredicted behaviour and therefore results in a RuntimeException
 * (only detected via filename pattern so will not catch all cases).
 */
public  class InsertIntoParquetTable extends org.apache.spark.sql.execution.SparkPlan implements org.apache.spark.sql.execution.UnaryNode, org.apache.spark.mapreduce.SparkHadoopMapReduceUtil, scala.Product, scala.Serializable {
  public  org.apache.spark.sql.parquet.ParquetRelation relation () { throw new RuntimeException(); }
  public  org.apache.spark.sql.execution.SparkPlan child () { throw new RuntimeException(); }
  public  boolean overwrite () { throw new RuntimeException(); }
  // not preceding
  public   InsertIntoParquetTable (org.apache.spark.sql.parquet.ParquetRelation relation, org.apache.spark.sql.execution.SparkPlan child, boolean overwrite) { throw new RuntimeException(); }
  /**
   * Inserts all rows into the Parquet file.
   */
  public  org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.expressions.Row> execute () { throw new RuntimeException(); }
  public  scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output () { throw new RuntimeException(); }
  /**
   * Stores the given Row RDD as a Hadoop file.
   * <p>
   * Note: We cannot use <code></code>saveAsNewAPIHadoopFile<code></code> from {@link org.apache.spark.rdd.PairRDDFunctions}
   * together with {@link org.apache.spark.util.MutablePair} because <code></code>PairRDDFunctions<code></code> uses
   * <code></code>Tuple2<code></code> and not <code></code>Product2<code></code>. Also, we want to allow appending files to an existing
   * directory and need to determine which was the largest written file index before starting to
   * write.
   * <p>
   * @param rdd The {@link org.apache.spark.rdd.RDD} to writer
   * @param path The directory to write to.
   * @param conf A {@link org.apache.hadoop.conf.Configuration}.
   */
  private  void saveAsHadoopFile (org.apache.spark.rdd.RDD<org.apache.spark.sql.catalyst.expressions.Row> rdd, java.lang.String path, org.apache.hadoop.conf.Configuration conf) { throw new RuntimeException(); }
}
